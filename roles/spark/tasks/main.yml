---
# tasks file for spark
# After installed go to http://hadoopmaster:4040/jobs/ to see the jobs executed
# When Starting spark use the following format
# spark-shell  --master  spark://hadoopmaster:7077 --jars /usr/local/hive/lib/mysql-connector-java.jar
- include_vars: "{{ nodesfile }}"
- name: stat /tmp/spark-2.1.0-bin-hadoop2.7.tgz
  stat: path=/tmp/spark-2.1.0-bin-hadoop2.7.tgz
  register: spark_binary_stat

# trying out spark 2.0.0
# http://d3kbcqa49mib13.cloudfront.net/spark-2.0.0-preview-bin-hadoop2.7.tgz

- name: Downloading Apache Spark
  get_url: url=http://d3kbcqa49mib13.cloudfront.net/spark-2.1.0-bin-hadoop2.7.tgz  dest=/tmp/spark-2.1.0-bin-hadoop2.7.tgz
  when: spark_binary_stat.stat.exists == False

- name: rename file
  command: chown hadoop:hadoop /tmp/spark-2.1.0-bin-hadoop2.7.tgz

- name: uncompress spark tarball and place in /usr/local/
  command: tar -zxf /tmp/spark-2.1.0-bin-hadoop2.7.tgz -C /usr/local

- name: rename file
  command: creates="/usr/local/spark" mv /usr/local/spark-2.1.0-bin-hadoop2.7/  /usr/local/spark

- name: Change owner to hadoop for all files under SPARK_HOME
  command: chown -R hadoop:hadoop  /usr/local/spark

- name: copying spark slaves conf
  template: src=slaves.j2 dest=/usr/local/spark/conf/slaves owner={{ hadoop_user }} group={{ hadoop_group }}
  when: api_hostname == "hadoopmaster"
  
- name: setup spark property files
  template: src={{ item.name }}.j2 dest=/usr/local/spark/conf/{{ item.name }} owner={{ hadoop_user }} group={{ hadoop_group }}
  with_items:
    - { name: spark-defaults.conf }
    - { name: spark-env.sh  }
    - { name: metrics.properties  }
